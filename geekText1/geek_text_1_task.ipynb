{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"geek_text_1_task.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMIGyMHecHV+mW5DUzTImhU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"yg9ImASDXueP","executionInfo":{"status":"ok","timestamp":1602489029104,"user_tz":-180,"elapsed":60118,"user":{"displayName":"alan neumann","photoUrl":"","userId":"13302226567094501168"}},"outputId":"31690a6f-3cc9-47db-c63c-3cf7052df75c","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive \n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TcrFjKWObr3G","executionInfo":{"status":"ok","timestamp":1602491572898,"user_tz":-180,"elapsed":3494,"user":{"displayName":"alan neumann","photoUrl":"","userId":"13302226567094501168"}},"outputId":"ec6c0402-c116-4d10-9140-c686380968c5","colab":{"base_uri":"https://localhost:8080/","height":105}},"source":["import pandas as pd\n","from html.parser import HTMLParser\n","import random\n","import re\n","import nltk\n","from nltk.tokenize import word_tokenize, wordpunct_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer, WordNetLemmatizer\n","\n","if 1:\n","    nltk.download('punkt')\n","    nltk.download('stopwords')\n","    !pip install stop-words\n","\n","from stop_words import get_stop_words"],"execution_count":71,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","Requirement already satisfied: stop-words in /usr/local/lib/python3.6/dist-packages (2018.7.23)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FokaMgUvbfTH","executionInfo":{"status":"ok","timestamp":1602492506799,"user_tz":-180,"elapsed":975,"user":{"displayName":"alan neumann","photoUrl":"","userId":"13302226567094501168"}}},"source":["myPath = 'drive/My Drive/geek_text/_1'\n","\n","trainDf = pd.read_csv(myPath/train_tweets.csv')\n","testDf = pd.read_csv('myPath/test_tweets.csv')"],"execution_count":101,"outputs":[]},{"cell_type":"code","metadata":{"id":"LaEqYcHabtrk","executionInfo":{"status":"ok","timestamp":1602489032905,"user_tz":-180,"elapsed":63895,"user":{"displayName":"alan neumann","photoUrl":"","userId":"13302226567094501168"}},"outputId":"3faa6f01-2052-4f96-ca69-ba41094a7135","colab":{"base_uri":"https://localhost:8080/","height":197}},"source":["trainDf.head()"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>label</th>\n","      <th>tweet</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>@user when a father is dysfunctional and is s...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>@user @user thanks for #lyft credit i can't us...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>bihday your majesty</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>#model   i love u take with u all the time in ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>factsguide: society now    #motivation</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id  label                                              tweet\n","0   1      0   @user when a father is dysfunctional and is s...\n","1   2      0  @user @user thanks for #lyft credit i can't us...\n","2   3      0                                bihday your majesty\n","3   4      0  #model   i love u take with u all the time in ...\n","4   5      0             factsguide: society now    #motivation"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"SD_uFC5Xb67Q"},"source":["\n","1. Заменим html-сущности (к примеру: < > &). \"<\" заменим на “<” и \"&\" заменим на “&”)\"\"\". Сделаем это с помощью HTMLParser.unescape(). Всю предобработку делаем в новом столбце 'clean_tweet'"]},{"cell_type":"code","metadata":{"id":"9koI_Jz4b0-B","executionInfo":{"status":"ok","timestamp":1602489076318,"user_tz":-180,"elapsed":107299,"user":{"displayName":"alan neumann","photoUrl":"","userId":"13302226567094501168"}},"outputId":"eea10864-a938-42bf-f8d5-0d19864c90ce","colab":{"base_uri":"https://localhost:8080/","height":961}},"source":["trainDf['crean_tweet'] = None\n","for i in range(trainDf.tweet.shape[0]):\n","    trainDf.loc[trainDf.index == i, 'clean_tweet'] = HTMLParser().unescape(\n","        trainDf.loc[trainDf.index == i, 'tweet']\n","    )\n","\n","# checking result\n","for i in random.sample(range(trainDf.shape[0]), 10):\n","    print(trainDf.loc[trainDf.index == i, 'tweet'])\n","    print(trainDf.loc[trainDf.index == i, 'clean_tweet'])\n","    print()"],"execution_count":5,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: The unescape method is deprecated and will be removed in 3.5, use html.unescape() instead.\n","  after removing the cwd from sys.path.\n"],"name":"stderr"},{"output_type":"stream","text":["2458    (the huffington post) a very   #father's day t...\n","Name: tweet, dtype: object\n","2458    (the huffington post) a very   #father's day t...\n","Name: clean_tweet, dtype: object\n","\n","7759    so tonight, spagetti bolognaise and scrabble. ...\n","Name: tweet, dtype: object\n","7759    so tonight, spagetti bolognaise and scrabble. ...\n","Name: clean_tweet, dtype: object\n","\n","19135    smile it's friday #weekend #friday #friends #e...\n","Name: tweet, dtype: object\n","19135    smile it's friday #weekend #friday #friends #e...\n","Name: clean_tweet, dtype: object\n","\n","15008    lights on and now lights off.. thank you @user...\n","Name: tweet, dtype: object\n","15008    lights on and now lights off.. thank you @user...\n","Name: clean_tweet, dtype: object\n","\n","10179    @user @user just looked at your timeline.  not...\n","Name: tweet, dtype: object\n","10179    @user @user just looked at your timeline.  not...\n","Name: clean_tweet, dtype: object\n","\n","22490     @user new #storytime    third wheel?  #thirdw...\n","Name: tweet, dtype: object\n","22490     @user new #storytime    third wheel?  #thirdw...\n","Name: clean_tweet, dtype: object\n","\n","19458     @user   to hear of #wendyhuntbatch's passing ...\n","Name: tweet, dtype: object\n","19458     @user   to hear of #wendyhuntbatch's passing ...\n","Name: clean_tweet, dtype: object\n","\n","28666    back in munich!!! #lmu   #missszeged\n","Name: tweet, dtype: object\n","28666    back in munich!!! #lmu   #missszeged\n","Name: clean_tweet, dtype: object\n","\n","3740    happy hump day! friday is on the horizon ladie...\n","Name: tweet, dtype: object\n","3740    happy hump day! friday is on the horizon ladie...\n","Name: clean_tweet, dtype: object\n","\n","5534    #beutiful poetry   poetry  #2 lines poetry  #e...\n","Name: tweet, dtype: object\n","5534    #beutiful poetry   poetry  #2 lines poetry  #e...\n","Name: clean_tweet, dtype: object\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"O8zSgQhRqfMP"},"source":["2. Удалим @user из всех твитов с помощью паттерна \"@[\\w]*\". Для этого создадим функцию:\n","для того, чтобы найти все вхождения паттерна в тексте, необходимо использовать re.findall(pattern, input_txt)\n","для для замены @user на пробел, необходимо использовать re.sub() при применении функции необходимо использовать np.vectorize(function).\n"]},{"cell_type":"code","metadata":{"id":"r4VP-8Mxck8A","executionInfo":{"status":"ok","timestamp":1602489119459,"user_tz":-180,"elapsed":150434,"user":{"displayName":"alan neumann","photoUrl":"","userId":"13302226567094501168"}},"outputId":"e61a0c3a-1339-4688-c6e0-0cac6f5d5daf","colab":{"base_uri":"https://localhost:8080/","height":905}},"source":["def cleanTweet(pattern, inputText):\n","    clean = re.compile(pattern)\n","    cleanTweet = re.sub(clean, '', inputText)\n","\n","    return cleanTweet\n","\n","pattern = '@[\\w]*'\n","\n","for i in range(trainDf.tweet.shape[0]):\n","    trainDf.loc[trainDf.index == i, 'clean_tweet'] = cleanTweet(\n","        pattern,\n","        trainDf.loc[trainDf.index == i, 'clean_tweet'].values[0]\n","    )\n","\n","# checking result\n","for i in random.sample(range(trainDf.shape[0]), 10):\n","    print(trainDf.loc[trainDf.index == i, 'tweet'])\n","    print(trainDf.loc[trainDf.index == i, 'clean_tweet'])\n","    print()"],"execution_count":6,"outputs":[{"output_type":"stream","text":["14297    @user we might be going to hell, but we're not...\n","Name: tweet, dtype: object\n","14297     we might be going to hell, but we're not surp...\n","Name: clean_tweet, dtype: object\n","\n","20966    what's the final result?  #mindset   #joy #pro...\n","Name: tweet, dtype: object\n","20966    what's the final result?  #mindset   #joy #pro...\n","Name: clean_tweet, dtype: object\n","\n","5311    its the hae tym u knw that life is sho no matt...\n","Name: tweet, dtype: object\n","5311    its the hae tym u knw that life is sho no matt...\n","Name: clean_tweet, dtype: object\n","\n","18700    finally new car!!  #peugeot   \n","Name: tweet, dtype: object\n","18700    finally new car!!  #peugeot   \n","Name: clean_tweet, dtype: object\n","\n","18663    my heas and prayers go out to all victims of o...\n","Name: tweet, dtype: object\n","18663    my heas and prayers go out to all victims of o...\n","Name: clean_tweet, dtype: object\n","\n","23147    tonight's the night!  headed to see @user @use...\n","Name: tweet, dtype: object\n","23147    tonight's the night!  headed to see   in ptbo,...\n","Name: clean_tweet, dtype: object\n","\n","30997    waking up in the morning has been so easy late...\n","Name: tweet, dtype: object\n","30997    waking up in the morning has been so easy late...\n","Name: clean_tweet, dtype: object\n","\n","24377    about to see @user in conce!!!  \n","Name: tweet, dtype: object\n","24377    about to see  in conce!!!  \n","Name: clean_tweet, dtype: object\n","\n","18800    islamist murdering lgbtq americans? how will t...\n","Name: tweet, dtype: object\n","18800    islamist murdering lgbtq americans? how will t...\n","Name: clean_tweet, dtype: object\n","\n","14815    our cars broke smh  \n","Name: tweet, dtype: object\n","14815    our cars broke smh  \n","Name: clean_tweet, dtype: object\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Wt1tjDwU7o3Z"},"source":["3. Изменим регистр твитов на нижний с помощью .lower()."]},{"cell_type":"code","metadata":{"id":"NWGKPdz_eOzo","executionInfo":{"status":"ok","timestamp":1602489161964,"user_tz":-180,"elapsed":192927,"user":{"displayName":"alan neumann","photoUrl":"","userId":"13302226567094501168"}},"outputId":"1579ca99-fb5a-4106-acdb-fd5b39f882fd","colab":{"base_uri":"https://localhost:8080/","height":372}},"source":["for i in range(trainDf.tweet.shape[0]):\n","    trainDf.loc[trainDf.index == i, 'clean_tweet'] = trainDf.loc[trainDf.index == i, 'clean_tweet'].values[0].lower()\n","\n","# checking result\n","for i in random.sample(range(trainDf.shape[0]), 10):\n","    print(trainDf.loc[trainDf.index == i, 'clean_tweet'])"],"execution_count":7,"outputs":[{"output_type":"stream","text":["14237    no matter how you pick it.   #flagday #flagday...\n","Name: clean_tweet, dtype: object\n","4526    i may finally get to go to my first  event thi...\n","Name: clean_tweet, dtype: object\n","30064    i #love diablo so much. he's my #boo.. #dog #r...\n","Name: clean_tweet, dtype: object\n","25895    i have begun something #new on this page &amp;...\n","Name: clean_tweet, dtype: object\n","19150    car wash with not so happy endingð¢ #carwash...\n","Name: clean_tweet, dtype: object\n","14507      niggas would fck over they boy just to hit s...\n","Name: clean_tweet, dtype: object\n","15020     â #oil under pressure, eyeing to test $47.0...\n","Name: clean_tweet, dtype: object\n","7490      #annoyed   #annoyed. all day everyday. \n","Name: clean_tweet, dtype: object\n","28181    - - #never become   when #life pulls u back ð...\n","Name: clean_tweet, dtype: object\n","283    never been more appropriate: i have zero idea ...\n","Name: clean_tweet, dtype: object\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Mpyz5F6d8cFQ"},"source":["4. Заменим сокращения с апострофами (пример: ain't, can't) на пробел, используя apostrophe_dict. Для этого необходимо сделать функцию: для каждого слова в тексте проверить (for word in text.split()), если слово есть в словаре apostrophe_dict в качестве ключа (сокращенного слова), то заменить ключ на значение (полную версию слова)."]},{"cell_type":"code","metadata":{"id":"sOy3RzxX52jc","executionInfo":{"status":"ok","timestamp":1602489161965,"user_tz":-180,"elapsed":192925,"user":{"displayName":"alan neumann","photoUrl":"","userId":"13302226567094501168"}}},"source":["apostrophe_dict = {\n","    \"ain't\": \"am not / are not\",\n","    \"aren't\": \"are not / am not\",\n","    \"can't\": \"cannot\",\n","    \"can't've\": \"cannot have\",\n","    \"'cause\": \"because\",\n","    \"could've\": \"could have\",\n","    \"couldn't\": \"could not\",\n","    \"couldn't've\": \"could not have\",\n","    \"didn't\": \"did not\",\n","    \"doesn't\": \"does not\",\n","    \"don't\": \"do not\",\n","    \"hadn't\": \"had not\",\n","    \"hadn't've\": \"had not have\",\n","    \"hasn't\": \"has not\",\n","    \"haven't\": \"have not\",\n","    \"he'd\": \"he had / he would\",\n","    \"he'd've\": \"he would have\",\n","    \"he'll\": \"he shall / he will\",\n","    \"he'll've\": \"he shall have / he will have\",\n","    \"he's\": \"he has / he is\",\n","    \"how'd\": \"how did\",\n","    \"how'd'y\": \"how do you\",\n","    \"how'll\": \"how will\",\n","    \"how's\": \"how has / how is\",\n","    \"i'd\": \"I had / I would\",\n","    \"i'd've\": \"I would have\",\n","    \"i'll\": \"I shall / I will\",\n","    \"i'll've\": \"I shall have / I will have\",\n","    \"i'm\": \"I am\",\n","    \"i've\": \"I have\",\n","    \"isn't\": \"is not\",\n","    \"it'd\": \"it had / it would\",\n","    \"it'd've\": \"it would have\",\n","    \"it'll\": \"it shall / it will\",\n","    \"it'll've\": \"it shall have / it will have\",\n","    \"it's\": \"it has / it is\",\n","    \"let's\": \"let us\",\n","    \"ma'am\": \"madam\",\n","    \"mayn't\": \"may not\",\n","    \"might've\": \"might have\",\n","    \"mightn't\": \"might not\",\n","    \"mightn't've\": \"might not have\",\n","    \"must've\": \"must have\",\n","    \"mustn't\": \"must not\",\n","    \"mustn't've\": \"must not have\",\n","    \"needn't\": \"need not\",\n","    \"needn't've\": \"need not have\",\n","    \"o'clock\": \"of the clock\",\n","    \"oughtn't\": \"ought not\",\n","    \"oughtn't've\": \"ought not have\",\n","    \"shan't\": \"shall not\",\n","    \"sha'n't\": \"shall not\",\n","    \"shan't've\": \"shall not have\",\n","    \"she'd\": \"she had / she would\",\n","    \"she'd've\": \"she would have\",\n","    \"she'll\": \"she shall / she will\",\n","    \"she'll've\": \"she shall have / she will have\",\n","    \"she's\": \"she has / she is\",\n","    \"should've\": \"should have\",\n","    \"shouldn't\": \"should not\",\n","    \"shouldn't've\": \"should not have\",\n","    \"so've\": \"so have\",\n","    \"so's\": \"so as / so is\",\n","    \"that'd\": \"that would / that had\",\n","    \"that'd've\": \"that would have\",\n","    \"that's\": \"that has / that is\",\n","    \"there'd\": \"there had / there would\",\n","    \"there'd've\": \"there would have\",\n","    \"there's\": \"there has / there is\",\n","    \"they'd\": \"they had / they would\",\n","    \"they'd've\": \"they would have\",\n","    \"they'll\": \"they shall / they will\",\n","    \"they'll've\": \"they shall have / they will have\",\n","    \"they're\": \"they are\",\n","    \"they've\": \"they have\",\n","    \"to've\": \"to have\",\n","    \"wasn't\": \"was not\",\n","    \"we'd\": \"we had / we would\",\n","    \"we'd've\": \"we would have\",\n","    \"we'll\": \"we will\",\n","    \"we'll've\": \"we will have\",\n","    \"we're\": \"we are\",\n","    \"we've\": \"we have\",\n","    \"weren't\": \"were not\",\n","    \"what'll\": \"what shall / what will\",\n","    \"what'll've\": \"what shall have / what will have\",\n","    \"what're\": \"what are\",\n","    \"what's\": \"what has / what is\",\n","    \"what've\": \"what have\",\n","    \"when's\": \"when has / when is\",\n","    \"when've\": \"when have\",\n","    \"where'd\": \"where did\",\n","    \"where's\": \"where has / where is\",\n","    \"where've\": \"where have\",\n","    \"who'll\": \"who shall / who will\",\n","    \"who'll've\": \"who shall have / who will have\",\n","    \"who's\": \"who has / who is\",\n","    \"who've\": \"who have\",\n","    \"why's\": \"why has / why is\",\n","    \"why've\": \"why have\",\n","    \"will've\": \"will have\",\n","    \"won't\": \"will not\",\n","    \"won't've\": \"will not have\",\n","    \"would've\": \"would have\",\n","    \"wouldn't\": \"would not\",\n","    \"wouldn't've\": \"would not have\",\n","    \"y'all\": \"you all\",\n","    \"y'all'd\": \"you all would\",\n","    \"y'all'd've\": \"you all would have\",\n","    \"y'all're\": \"you all are\",\n","    \"y'all've\": \"you all have\",\n","    \"you'd\": \"you had / you would\",\n","    \"you'd've\": \"you would have\",\n","    \"you'll\": \"you shall / you will\",\n","    \"you'll've\": \"you shall have / you will have\",\n","    \"you're\": \"you are\",\n","    \"you've\": \"you have\"\n","}"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"AmqrWnK3n8ZZ","executionInfo":{"status":"ok","timestamp":1602489205368,"user_tz":-180,"elapsed":236324,"user":{"displayName":"alan neumann","photoUrl":"","userId":"13302226567094501168"}}},"source":["for i in range(trainDf.tweet.shape[0]):\n","    aTweet = trainDf.loc[trainDf.index == i, 'clean_tweet'].values[0]\n","    trainDf.loc[trainDf.index == i, 'clean_tweet'] = ' '.join(\n","        [apostrophe_dict.get(j, j) for j in aTweet.split()]\n","    )"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nx7g8CegDTHS"},"source":["5. Заменим сокращения на их полные формы, используя short_word_dict. Для этого воспользуемся функцией, используемой в предыдущем пункте."]},{"cell_type":"code","metadata":{"id":"IY51dGE2C5lq","executionInfo":{"status":"ok","timestamp":1602489205371,"user_tz":-180,"elapsed":236324,"user":{"displayName":"alan neumann","photoUrl":"","userId":"13302226567094501168"}}},"source":["short_word_dict = {\n","    \"121\": \"one to one\",\n","    \"a/s/l\": \"age, sex, location\",\n","    \"adn\": \"any day now\",\n","    \"afaik\": \"as far as I know\",\n","    \"afk\": \"away from keyboard\",\n","    \"aight\": \"alright\",\n","    \"alol\": \"actually laughing out loud\",\n","    \"b4\": \"before\",\n","    \"b4n\": \"bye for now\",\n","    \"bak\": \"back at the keyboard\",\n","    \"bf\": \"boyfriend\",\n","    \"bff\": \"best friends forever\",\n","    \"bfn\": \"bye for now\",\n","    \"bg\": \"big grin\",\n","    \"bta\": \"but then again\",\n","    \"btw\": \"by the way\",\n","    \"cid\": \"crying in disgrace\",\n","    \"cnp\": \"continued in my next post\",\n","    \"cp\": \"chat post\",\n","    \"cu\": \"see you\",\n","    \"cul\": \"see you later\",\n","    \"cul8r\": \"see you later\",\n","    \"cya\": \"bye\",\n","    \"cyo\": \"see you online\",\n","    \"dbau\": \"doing business as usual\",\n","    \"fud\": \"fear, uncertainty, and doubt\",\n","    \"fwiw\": \"for what it's worth\",\n","    \"fyi\": \"for your information\",\n","    \"g\": \"grin\",\n","    \"g2g\": \"got to go\",\n","    \"ga\": \"go ahead\",\n","    \"gal\": \"get a life\",\n","    \"gf\": \"girlfriend\",\n","    \"gfn\": \"gone for now\",\n","    \"gmbo\": \"giggling my butt off\",\n","    \"gmta\": \"great minds think alike\",\n","    \"h8\": \"hate\",\n","    \"hagn\": \"have a good night\",\n","    \"hdop\": \"help delete online predators\",\n","    \"hhis\": \"hanging head in shame\",\n","    \"iac\": \"in any case\",\n","    \"ianal\": \"I am not a lawyer\",\n","    \"ic\": \"I see\",\n","    \"idk\": \"I don't know\",\n","    \"imao\": \"in my arrogant opinion\",\n","    \"imnsho\": \"in my not so humble opinion\",\n","    \"imo\": \"in my opinion\",\n","    \"iow\": \"in other words\",\n","    \"ipn\": \"I’m posting naked\",\n","    \"irl\": \"in real life\",\n","    \"jk\": \"just kidding\",\n","    \"l8r\": \"later\",\n","    \"ld\": \"later, dude\",\n","    \"ldr\": \"long distance relationship\",\n","    \"llta\": \"lots and lots of thunderous applause\",\n","    \"lmao\": \"laugh my ass off\",\n","    \"lmirl\": \"let's meet in real life\",\n","    \"lol\": \"laugh out loud\",\n","    \"ltr\": \"longterm relationship\",\n","    \"lulab\": \"love you like a brother\",\n","    \"lulas\": \"love you like a sister\",\n","    \"luv\": \"love\",\n","    \"m/f\": \"male or female\",\n","    \"m8\": \"mate\",\n","    \"milf\": \"mother I would like to fuck\",\n","    \"oll\": \"online love\",\n","    \"omg\": \"oh my god\",\n","    \"otoh\": \"on the other hand\",\n","    \"pir\": \"parent in room\",\n","    \"ppl\": \"people\",\n","    \"r\": \"are\",\n","    \"rofl\": \"roll on the floor laughing\",\n","    \"rpg\": \"role playing games\",\n","    \"ru\": \"are you\",\n","    \"shid\": \"slaps head in disgust\",\n","    \"somy\": \"sick of me yet\",\n","    \"sot\": \"short of time\",\n","    \"thanx\": \"thanks\",\n","    \"thx\": \"thanks\",\n","    \"ttyl\": \"talk to you later\",\n","    \"u\": \"you\",\n","    \"ur\": \"you are\",\n","    \"uw\": \"you’re welcome\",\n","    \"wb\": \"welcome back\",\n","    \"wfm\": \"works for me\",\n","    \"wibni\": \"wouldn't it be nice if\",\n","    \"wtf\": \"what the fuck\",\n","    \"wtg\": \"way to go\",\n","    \"wtgp\": \"want to go private\",\n","    \"ym\": \"young man\",\n","    \"gr8\": \"great\"\n","}"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"yTWxdKwNAVdC","executionInfo":{"status":"ok","timestamp":1602489248813,"user_tz":-180,"elapsed":279762,"user":{"displayName":"alan neumann","photoUrl":"","userId":"13302226567094501168"}}},"source":["for i in range(trainDf.tweet.shape[0]):\n","    aTweet = trainDf.loc[trainDf.index == i, 'clean_tweet'].values[0]\n","    trainDf.loc[trainDf.index == i, 'clean_tweet'] = ' '.join(\n","        [short_word_dict.get(j, j) for j in aTweet.split()]\n","    )"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"asfRqx88Dsym"},"source":["6. Заменим эмотиконы (пример: \":)\" = \"happy\") на пробелы, используя emoticon_dict. Для этого воспользуемся функцией, используемой в предыдущем пункте."]},{"cell_type":"code","metadata":{"id":"Bmk8NfD3DlVB","executionInfo":{"status":"ok","timestamp":1602489248818,"user_tz":-180,"elapsed":279762,"user":{"displayName":"alan neumann","photoUrl":"","userId":"13302226567094501168"}}},"source":["emoticon_dict = {\n","    \":)\": \"happy\",\n","    \":‑)\": \"happy\",\n","    \":-]\": \"happy\",\n","    \":-3\": \"happy\",\n","    \":->\": \"happy\",\n","    \"8-)\": \"happy\",\n","    \":-}\": \"happy\",\n","    \":o)\": \"happy\",\n","    \":c)\": \"happy\",\n","    \":^)\": \"happy\",\n","    \"=]\": \"happy\",\n","    \"=)\": \"happy\",\n","    \"<3\": \"happy\",\n","    \":-(\": \"sad\",\n","    \":(\": \"sad\",\n","    \":c\": \"sad\",\n","    \":<\": \"sad\",\n","    \":[\": \"sad\",\n","    \">:[\": \"sad\",\n","    \":{\": \"sad\",\n","    \">:(\": \"sad\",\n","    \":-c\": \"sad\",\n","    \":-< \": \"sad\",\n","    \":-[\": \"sad\",\n","    \":-||\": \"sad\"\n","}"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"_moYTYbNIorR","executionInfo":{"status":"ok","timestamp":1602489291888,"user_tz":-180,"elapsed":322829,"user":{"displayName":"alan neumann","photoUrl":"","userId":"13302226567094501168"}}},"source":["for i in range(trainDf.tweet.shape[0]):\n","    aTweet = trainDf.loc[trainDf.index == i, 'clean_tweet'].values[0]\n","    trainDf.loc[trainDf.index == i, 'clean_tweet'] = ' '.join(\n","        [emoticon_dict.get(j, j) for j in aTweet.split()]\n","    )"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kh_t-U9qJZz1"},"source":["7. Заменим пунктуацию на пробелы, используя re.sub() и паттерн r'[^\\w\\s]'."]},{"cell_type":"code","metadata":{"id":"SBa51bNVJecz","executionInfo":{"status":"ok","timestamp":1602489334622,"user_tz":-180,"elapsed":365560,"user":{"displayName":"alan neumann","photoUrl":"","userId":"13302226567094501168"}}},"source":["def cleanTweet(pattern, inputText):\n","    clean = re.compile(pattern)\n","    cleanTweet = re.sub(clean, ' ', inputText)\n","\n","    return cleanTweet\n","\n","pattern = '[^\\w\\s]'\n","\n","for i in range(trainDf.tweet.shape[0]):\n","    trainDf.loc[trainDf.index == i, 'clean_tweet'] = cleanTweet(\n","        pattern,\n","        trainDf.loc[trainDf.index == i, 'clean_tweet'].values[0]\n","    )\n"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PrSEtKciJ-sP"},"source":["8. Заменим спец. символы на пробелы, используя re.sub() и паттерн r'[^a-zA-Z0-9]'"]},{"cell_type":"code","metadata":{"id":"VwCON6JRKKDX","executionInfo":{"status":"ok","timestamp":1602489378337,"user_tz":-180,"elapsed":409271,"user":{"displayName":"alan neumann","photoUrl":"","userId":"13302226567094501168"}}},"source":["def cleanTweet(pattern, inputText):\n","    clean = re.compile(pattern)\n","    cleanTweet = re.sub(clean, ' ', inputText)\n","\n","    return cleanTweet\n","\n","pattern = '[^a-zA-Z0-9]'\n","\n","for i in range(trainDf.tweet.shape[0]):\n","    trainDf.loc[trainDf.index == i, 'clean_tweet'] = cleanTweet(\n","        pattern,\n","        trainDf.loc[trainDf.index == i, 'clean_tweet'].values[0]\n","    )\n"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CjTPuLJNKqhp"},"source":["9. Заменим числа на пробелы, используя re.sub() и паттерн r'[^a-zA-Z]'."]},{"cell_type":"code","metadata":{"id":"sQr7edHEK_8I","executionInfo":{"status":"ok","timestamp":1602489421046,"user_tz":-180,"elapsed":451977,"user":{"displayName":"alan neumann","photoUrl":"","userId":"13302226567094501168"}}},"source":["def cleanTweet(pattern, inputText):\n","    clean = re.compile(pattern)\n","    cleanTweet = re.sub(clean, ' ', inputText)\n","\n","    return cleanTweet\n","\n","pattern = '[^a-zA-Z]'\n","\n","for i in range(trainDf.tweet.shape[0]):\n","    trainDf.loc[trainDf.index == i, 'clean_tweet'] = cleanTweet(\n","        pattern,\n","        trainDf.loc[trainDf.index == i, 'clean_tweet'].values[0]\n","    )"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0nQ5P14hLy5e"},"source":["10. Удалим из текста слова длиной в 1 символ, используя ' '.join([w for w in x.split() if len(w)>1]).\n"]},{"cell_type":"code","metadata":{"id":"RDmoZzzdJzmo","executionInfo":{"status":"ok","timestamp":1602489463411,"user_tz":-180,"elapsed":494339,"user":{"displayName":"alan neumann","photoUrl":"","userId":"13302226567094501168"}}},"source":["for i in range(trainDf.tweet.shape[0]):\n","    aTweet = trainDf.loc[trainDf.index == i, 'clean_tweet'].values[0]\n","    trainDf.loc[trainDf.index == i, 'clean_tweet'] = ' '.join(\n","        [w for w in aTweet.split() if len(w) > 1]\n","    )"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1SFd7pkGP3We"},"source":["11. Поделим твиты на токены с помощью nltk.tokenize.word_tokenize, создав новый столбец 'tweet_token'."]},{"cell_type":"code","metadata":{"id":"eCOD11OiMYz9","executionInfo":{"status":"ok","timestamp":1602489856663,"user_tz":-180,"elapsed":22846,"user":{"displayName":"alan neumann","photoUrl":"","userId":"13302226567094501168"}},"outputId":"7568c266-ece8-4533-b9f5-b9747816dc70","colab":{"base_uri":"https://localhost:8080/","height":427}},"source":["trainDf['tweet_token'] = 0\n","\n","for i in range(trainDf.tweet.shape[0]):\n","    aTweet = trainDf.loc[trainDf.index == i, 'clean_tweet'].values[0]\n","    trainDf.tweet_token[i] = word_tokenize(aTweet)"],"execution_count":33,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  \"\"\"\n","/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  iloc._setitem_with_indexer(indexer, value)\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["0        [when, father, is, dysfunctional, and, is, so,...\n","1        [thanks, for, lyft, credit, can, not, use, cau...\n","2                                  [bihday, your, majesty]\n","3        [model, love, you, take, with, you, all, the, ...\n","4                   [factsguide, society, now, motivation]\n","                               ...                        \n","31957                              [ate, isz, that, youuu]\n","31958    [to, see, nina, turner, on, the, airwaves, try...\n","31959    [listening, to, sad, songs, on, monday, mornin...\n","31960    [sikh, temple, vandalised, in, in, calgary, ws...\n","31961                       [thank, you, for, you, follow]\n","Name: tweet_token, Length: 31962, dtype: object"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"markdown","metadata":{"id":"R2_tO8tz2Aky"},"source":["12. Удалим стоп-слова из токенов, используя nltk.corpus.stopwords. Создадим столбец 'tweet_token_filtered' без стоп-слов."]},{"cell_type":"code","metadata":{"id":"xsvsddVhTSBn","executionInfo":{"status":"ok","timestamp":1602490766585,"user_tz":-180,"elapsed":16903,"user":{"displayName":"alan neumann","photoUrl":"","userId":"13302226567094501168"}},"outputId":"a515f307-2619-47b2-aae3-bd4da660f002","colab":{"base_uri":"https://localhost:8080/","height":214}},"source":["stopWords = set(stopwords.words('english'))\n","\n","trainDf['tweet_token_filtered'] = 0\n","\n","for i in range(trainDf.tweet.shape[0]):\n","    aToken = trainDf.loc[trainDf.index == i, 'tweet_token'].values[0]\n","    trainDf.tweet_token_filtered[i] = [w for w in aToken if not w in stopWords]"],"execution_count":63,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  import sys\n","/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  iloc._setitem_with_indexer(indexer, value)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"Mll38yJy7rVW"},"source":["13. Применим стемминг к токенам с помощью nltk.stem.PorterStemmer. Создадим столбец 'tweet_stemmed' после применения стемминга."]},{"cell_type":"code","metadata":{"id":"EsOCQvT49Zdq","executionInfo":{"status":"ok","timestamp":1602492251644,"user_tz":-180,"elapsed":14832,"user":{"displayName":"alan neumann","photoUrl":"","userId":"13302226567094501168"}},"outputId":"a538ac04-d3a0-4ca2-84c8-79e89ecfec6b","colab":{"base_uri":"https://localhost:8080/","height":214}},"source":["stemmer = PorterStemmer()\n","trainDf['tweet_stemmed'] = 0\n","for i in range(trainDf.shape[0]):\n","    trainDf.tweet_stemmed[i] = [stemmer.stem(w) for w in trainDf.tweet_token_filtered[i]]"],"execution_count":96,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  after removing the cwd from sys.path.\n","/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  iloc._setitem_with_indexer(indexer, value)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"XpnuDgSl-zvR"},"source":["14. Применим лемматизацию к токенам с помощью nltk.stem.wordnet.WordNetLemmatizer. Создадим столбец 'tweet_lemmatized' после применения лемматизации."]},{"cell_type":"code","metadata":{"id":"HGHF7MVNTVUB","executionInfo":{"status":"ok","timestamp":1602492423907,"user_tz":-180,"elapsed":14624,"user":{"displayName":"alan neumann","photoUrl":"","userId":"13302226567094501168"}},"outputId":"f119fbb3-f7de-41a2-e893-4191f97b1534","colab":{"base_uri":"https://localhost:8080/","height":214}},"source":["lemmatizer = WordNetLemmatizer()\n","trainDf['tweet_lemmatized'] = 0\n","for i in range(trainDf.shape[0]):\n","    trainDf.tweet_lemmatized[i] = [stemmer.stem(w) for w in trainDf.tweet_token_filtered[i]]"],"execution_count":98,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  after removing the cwd from sys.path.\n","/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  iloc._setitem_with_indexer(indexer, value)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"d2vDIl_1_ccD"},"source":["15. Сохраним результат предобработки в pickle-файл."]},{"cell_type":"code","metadata":{"id":"DN44O2gRTsEe","executionInfo":{"status":"ok","timestamp":1602492610624,"user_tz":-180,"elapsed":934,"user":{"displayName":"alan neumann","photoUrl":"","userId":"13302226567094501168"}}},"source":["trainDf.to_pickle(myPath+'trainProc.pkl')"],"execution_count":103,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u7Shf6cR31MT"},"source":[""]}]}